{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "start = dt.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from prophet import Prophet\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout,GRU\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Activation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    df = pd.read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "    df.set_index('Country/Region', inplace=True)\n",
    "    india_data = df.loc['India']\n",
    "    india_data = india_data.drop(['Province/State', 'Lat', 'Long'])\n",
    "    india_data = india_data.transpose()\n",
    "    india_data.columns = [\"Total Confirmed\"]\n",
    "    data = pd.DataFrame(india_data)\n",
    "    data['India'] = pd.to_numeric(data['India'], errors='coerce')\n",
    "    data1 = data.rename(columns={'India': 'Total Confirmed'})\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Confirmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/22/20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/23/20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/24/20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/25/20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/26/20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Total Confirmed\n",
       "1/22/20                0\n",
       "1/23/20                0\n",
       "1/24/20                0\n",
       "1/25/20                0\n",
       "1/26/20                0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA MAPE: 3.7593\n",
      "ARIMA MDAPE: 3.5425\n",
      "ARIMA SMAPE: 3.6546\n"
     ]
    }
   ],
   "source": [
    "def arima(data1):\n",
    "    train_len = (int)(0.8*len(data1))\n",
    "    df_train = data1.iloc[:train_len]\n",
    "    df_test = data1.iloc[train_len:]\n",
    "    model=ARIMA(df_train,order=(2,2,2))\n",
    "    model_fit=model.fit()\n",
    "    res=model_fit.predict(start = len(df_train), end = 1143, dynamic=True)\n",
    "    res = pd.DataFrame(res)\n",
    "    res.index = pd.to_datetime(res.index)\n",
    "    df_test.index = pd.to_datetime(df_test.index)\n",
    "    data1.index = pd.to_datetime(data1.index)\n",
    "    res = res[:-1]\n",
    "    MAPE = mean_absolute_percentage_error(df_test['Total Confirmed'].values, res['predicted_mean'].values)*100\n",
    "    mdape = np.median((np.abs(np.subtract(df_test['Total Confirmed'].values, res['predicted_mean'].values)/df_test['Total Confirmed'].values)))*100\n",
    "    smape = 100/len(df_test['Total Confirmed'].values) * np.sum(2 * np.abs(df_test['Total Confirmed'].values - res['predicted_mean'].values) / (np.abs(df_test['Total Confirmed'].values) + np.abs(res['predicted_mean'].values)))\n",
    "    print(\"ARIMA MAPE: {:.4f}\".format(MAPE))\n",
    "    print(\"ARIMA MDAPE: {:.4f}\".format(mdape))\n",
    "    print('ARIMA SMAPE: {:.4f}'.format(smape))\n",
    "    df = pd.read_csv('graph.csv')\n",
    "    df['ARIMA'] = res['predicted_mean'].values\n",
    "    df.to_csv('graph.csv', index=False)\n",
    "arima(data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 2s 7ms/step - loss: 0.1939 - val_loss: 0.5464\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.3028\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0603 - val_loss: 0.1424\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0662\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0361\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.0218\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0131\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0071\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0033\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0011\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 2.0953e-04\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 7.4109e-07\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 1.4798e-04\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 4.0757e-04\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 6.5109e-04\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 9.9644e-04 - val_loss: 8.3045e-04\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 8.4805e-04 - val_loss: 9.5190e-04\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 7.7687e-04 - val_loss: 0.0010\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 7.4943e-04 - val_loss: 0.0011\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 7.3702e-04 - val_loss: 0.0011\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 7.4267e-04 - val_loss: 0.0011\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 7.3795e-04 - val_loss: 0.0011\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "LSTM  MAPE: 3.8991\n",
      "LSTM  MDAPE: 3.9490\n",
      "LSTM  SMAPE: 3.8245\n"
     ]
    }
   ],
   "source": [
    "def fit_model(model, X_train, y_train, X_val, y_val):\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),\n",
    "                        batch_size=16, shuffle=False, callbacks=[early_stop])\n",
    "    return history\n",
    "\n",
    "def create_dataset (X, look_back = 1):\n",
    "  Xs,ys = [],[]\n",
    "  for i in range(len(X)-look_back):\n",
    "    v = X[i:i+look_back]\n",
    "    Xs.append(v)\n",
    "    ys.append(X[i+look_back])\n",
    "  return np.array(Xs),np.array(ys)\n",
    "\n",
    "def prediction(model,scaler,X_test):\n",
    "  prediction = model.predict(X_test)\n",
    "  prediction = prediction.reshape(-1, prediction.shape[-1])\n",
    "  prediction = scaler.inverse_transform(prediction)\n",
    "  return prediction\n",
    "\n",
    "def evaluate_prediction(predictions,actual,mod):\n",
    "  mape = mean_absolute_percentage_error(actual, predictions)*100\n",
    "  mdape = np.median((np.abs(np.subtract(actual, predictions)/ actual)))*100\n",
    "  smape = 100/len(actual) * np.sum(2 * np.abs(predictions-actual) / (np.abs(actual) + np.abs(predictions)))\n",
    "  print(mod,\" MAPE: {:.4f}\".format(mape))\n",
    "  print(mod, \" MDAPE: {:.4f}\".format(mdape))\n",
    "  print(mod, \" SMAPE: {:.4f}\".format(smape))\n",
    "\n",
    "def lst(df):\n",
    "    train_length = (int)(len(df)*0.8)\n",
    "    train = df[:train_length-1]\n",
    "    test = df[train_length-1:]\n",
    "    # print(len(test))\n",
    "    scaler = MinMaxScaler().fit(train)\n",
    "    train_scaled = scaler.transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    # print(len(test_scaled))\n",
    "    X_train,y_train = create_dataset(train_scaled,look_back = 1)\n",
    "    X_test,y_test = create_dataset(test_scaled,look_back = 1)\n",
    "    # print(len(y_test))\n",
    "    X_train, X_val = X_train[:int(len(X_train)*0.9)], X_train[int(len(X_train)*0.9):]\n",
    "    y_train, y_val = y_train[:int(len(y_train)*0.9)], y_train[int(len(y_train)*0.9):]\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(64, return_sequences = True, activation='relu', input_shape= [X_train.shape[1],X_train.shape[2]]))\n",
    "    model_lstm.add(Dense(1))\n",
    "    model_lstm.compile(optimizer='adam', loss='mse')\n",
    "    history_lstm = fit_model(model_lstm, X_train, y_train, X_val, y_val)\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "    y_train = scaler.inverse_transform(y_train)\n",
    "    prediction_lstm = prediction(model_lstm, scaler, X_test)\n",
    "    evaluate_prediction(prediction_lstm,y_test,\"LSTM\")\n",
    "    # print(len(prediction_lstm))\n",
    "    df = pd.read_csv('graph.csv')\n",
    "    df['LSTM'] = prediction_lstm\n",
    "    df.to_csv('graph.csv', index=False)\n",
    "lst(data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 5s 19ms/step - loss: 0.0097 - val_loss: 0.0028\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0067\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.0045\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0089 - val_loss: 5.8198e-05\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 2.3746e-04\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 1.9107e-04\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 1.6419e-06\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 3.0841e-04\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 4.2332e-04\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0027 - val_loss: 2.5941e-06\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 7.2976e-05\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 2.0762e-04\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 2.5172e-05\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0027 - val_loss: 2.7512e-07\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 9.2028e-04\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 6.8753e-05\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 8.2209e-04\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 8.4373e-06\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 4.1722e-05\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 5.3775e-04\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 3.7573e-04\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 4.8563e-05\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 1.2762e-04\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 2.6758e-04\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "GRU  MAPE: 1.4900\n",
      "GRU  MDAPE: 1.4825\n",
      "GRU  SMAPE: 1.4790\n"
     ]
    }
   ],
   "source": [
    "def fit_model(model, X_train, y_train, X_val, y_val):\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),\n",
    "                        batch_size=16, shuffle=False, callbacks=[early_stop])\n",
    "    return history\n",
    "\n",
    "def gru_model(df):\n",
    "    train_length = (int)(len(df)*0.8)\n",
    "    train = df[:train_length-1]\n",
    "    test = df[train_length-1:]\n",
    "    scaler = MinMaxScaler().fit(train)\n",
    "    train_scaled = scaler.transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    X_train,y_train = create_dataset(train_scaled,look_back = 1)\n",
    "    X_test,y_test = create_dataset(test_scaled,look_back = 1)\n",
    "    X_train, X_val = X_train[:int(len(X_train)*0.9)], X_train[int(len(X_train)*0.9):]\n",
    "    y_train, y_val = y_train[:int(len(y_train)*0.9)], y_train[int(len(y_train)*0.9):]\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(GRU(units = 64,return_sequences = True,\n",
    "                input_shape = [X_train.shape[1],X_train.shape[2]]))\n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(GRU(units = 64))\n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(Dense(units = 1))\n",
    "    model_gru.compile(optimizer = \"adam\",loss = \"mse\")\n",
    "    history_gru = fit_model(model_gru, X_train, y_train, X_val, y_val)\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "    y_train = scaler.inverse_transform(y_train)\n",
    "    prediction_gru = prediction(model_gru,scaler, X_test)\n",
    "    evaluate_prediction(prediction_gru,y_test,\"GRU\")\n",
    "    df = pd.read_csv('graph.csv')\n",
    "    df['GRU'] = prediction_gru\n",
    "    df.to_csv('graph.csv', index=False)\n",
    "gru_model(data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(914, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:58:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:58:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1143, 4)\n",
      "1143\n",
      "(1143, 1)\n",
      "         prophet_prediction\n",
      "1/22/20      -827217.038260\n",
      "1/23/20      -680664.845781\n",
      "1/24/20      -539671.319792\n",
      "1/25/20      -405410.222607\n",
      "1/26/20      -279644.662286\n",
      "         Total Confirmed  prophet_prediction\n",
      "1/22/20                0           -827218.0\n",
      "1/23/20                0           -680665.0\n",
      "1/24/20                0           -539672.0\n",
      "1/25/20                0           -405411.0\n",
      "1/26/20                0           -279645.0\n",
      "         Total Confirmed  prophet_prediction\n",
      "7/24/22         43905621          44003712.0\n",
      "7/25/22         43920451          44035211.0\n",
      "7/26/22         43938764          44072042.0\n",
      "7/27/22         43959321          44121218.0\n",
      "7/28/22         43979730          44163922.0\n",
      "(229, 2)\n",
      "Prophet MAPE: 15.4722\n",
      "Prophet MDAPE: 14.2306\n",
      "Prophet SMAPE: 13.8885\n"
     ]
    }
   ],
   "source": [
    "def prophet_model(data, skillset):\n",
    "    prophet_Data = data[[skillset]]\n",
    "    train_size = (int)(0.8*len(data))\n",
    "    new_percentage = train_size\n",
    "    train_prophet_Data = prophet_Data.head(new_percentage)\n",
    "    test_prophet_Data = prophet_Data.iloc[new_percentage:len(prophet_Data),:]\n",
    "    ts1 = pd.DataFrame({'ds': train_prophet_Data.index,\"y\": train_prophet_Data[skillset]})\n",
    "    prophet = Prophet()\n",
    "    print(ts1.shape)\n",
    "    prophet.fit(ts1)\n",
    "    future = prophet.make_future_dataframe(periods = len(data)-new_percentage, freq = 'D')\n",
    "    forecast = prophet.predict(future)\n",
    "    forecasted_values = forecast[['ds','yhat', 'yhat_lower','yhat_upper']]\n",
    "    print(forecasted_values.shape)\n",
    "    prophet_df = pd.DataFrame(forecasted_values[[\"yhat\"]])\n",
    "    a = list(prophet_Data.index)\n",
    "    print(len(a))\n",
    "    print(prophet_df.shape)\n",
    "    prophet_df1 = prophet_df.set_axis(a)\n",
    "    prophet_df1.rename(columns = {list(prophet_df1)[0]: \"prophet_prediction\"}, inplace=True)\n",
    "    print(prophet_df1.head())\n",
    "    final_data = pd.concat([prophet_Data[[skillset]],prophet_df1['prophet_prediction'].apply(np.floor)],axis=1)\n",
    "    print(final_data.head())\n",
    "    prophet_percent = train_size\n",
    "    prophet_metric_train = final_data.head(prophet_percent)\n",
    "    prophet_metric_test = final_data.iloc[prophet_percent: len(final_data),:]\n",
    "    print(prophet_metric_test.head())\n",
    "    print(prophet_metric_test.shape)\n",
    "    MAPE = mean_absolute_percentage_error(prophet_metric_test[skillset].values, prophet_metric_test[\"prophet_prediction\"].values)*100\n",
    "    mdape = np.median((np.abs(np.subtract(prophet_metric_test[skillset].values, prophet_metric_test[\"prophet_prediction\"].values)/prophet_metric_test[skillset].values)))*100\n",
    "    smape = 100/len(prophet_metric_test[skillset].values) * np.sum(2 * np.abs(prophet_metric_test[skillset].values - prophet_metric_test[\"prophet_prediction\"].values) / (np.abs(prophet_metric_test[skillset].values) + np.abs(prophet_metric_test[\"prophet_prediction\"].values)))\n",
    "    print(\"Prophet MAPE: {:.4f}\".format(MAPE))\n",
    "    print(\"Prophet MDAPE: {:.4f}\".format(mdape))\n",
    "    print('Prophet SMAPE: {:.4f}'.format(smape))\n",
    "    df = pd.read_csv('graph.csv')\n",
    "    df['prophet'] = prophet_metric_test[\"prophet_prediction\"].values\n",
    "    df.to_csv('graph.csv', index=False)\n",
    "prophet_model(data(), 'Total Confirmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "83/83 [==============================] - 1s 3ms/step - loss: 0.0511 - val_loss: 0.0015\n",
      "Epoch 2/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0019\n",
      "Epoch 3/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 5.8322e-04\n",
      "Epoch 4/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 5.8756e-04\n",
      "Epoch 5/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0035\n",
      "Epoch 6/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 5.5649e-04\n",
      "Epoch 7/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0084 - val_loss: 5.3542e-04\n",
      "Epoch 8/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 5.4225e-04\n",
      "Epoch 9/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0023\n",
      "Epoch 10/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 7.0568e-04\n",
      "Epoch 11/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 5.0316e-04\n",
      "Epoch 12/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 8.1452e-04\n",
      "Epoch 13/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 7.3226e-04\n",
      "Epoch 14/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 6.7426e-04\n",
      "Epoch 15/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.0013\n",
      "Epoch 16/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.0016\n",
      "Epoch 17/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 9.4297e-04\n",
      "Epoch 18/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 5.1869e-04\n",
      "Epoch 19/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 4.8791e-04\n",
      "Epoch 20/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 5.0497e-04\n",
      "Epoch 21/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 5.6348e-04\n",
      "Epoch 22/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 5.5540e-04\n",
      "Epoch 23/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.0010\n",
      "Epoch 24/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0012\n",
      "Epoch 25/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0044\n",
      "Epoch 26/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 4.9416e-04\n",
      "Epoch 27/100\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 9.3312e-04\n",
      "Epoch 28/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0022\n",
      "Epoch 29/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 5.1023e-04\n",
      "8/8 [==============================] - 0s 789us/step\n",
      "Test MAPE: 0.024572\n",
      "ARIMA+ANN  MAPE: 2.4572\n",
      "ARIMA+ANN  MDAPE: 2.5566\n",
      "ARIMA+ANN  SMAPE: 2.4680\n"
     ]
    }
   ],
   "source": [
    "def arima_ann(data1):\n",
    "    len_training = int(len(data1)*.8)\n",
    "    df_train,df_test = data1[:len_training], data1[len_training:]\n",
    "    predicted_list, error_list = [], []\n",
    "    model = ARIMA(df_train, order=(2, 2, 2))\n",
    "    model_fit = model.fit()\n",
    "    predicted_values = model_fit.predict(start = len(df_train), end = 1143, dynamic=True)\n",
    "    for t in range(len(df_test)):\n",
    "        obs = df_test.iloc[t]['Total Confirmed']\n",
    "        predicted_value = predicted_values[t]\n",
    "        error = abs(predicted_value - obs)\n",
    "        error_list.append(obs - predicted_value)\n",
    "        predicted_list.append(predicted_value)\n",
    "    training_error = model_fit.resid\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=1, kernel_initializer=\"uniform\", activation=\"tanh\"))\n",
    "    model.add(Dense(1, kernel_initializer=\"uniform\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    optimizer = RMSprop(learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    train_X,train_Y = [],[]\n",
    "    for i in range(0 , len(training_error) - 1):\n",
    "        train_X.append(training_error[i:i+1])\n",
    "        train_Y.append(training_error[i+1])\n",
    "    new_train_X,new_train_Y = [],[]\n",
    "    for i in train_X:\n",
    "        new_train_X.append(i)\n",
    "    for i in train_Y:\n",
    "        new_train_Y.append(i)\n",
    "    new_train_X = np.array(new_train_X)\n",
    "    new_train_Y = np.array(new_train_Y)\n",
    "\n",
    "    scaler_X = preprocessing.MinMaxScaler()\n",
    "    scaler_Y = preprocessing.MinMaxScaler()\n",
    "    new_train_X_scaled = scaler_X.fit_transform(new_train_X.reshape(-1,1))\n",
    "    new_train_Y_scaled = scaler_Y.fit_transform(new_train_Y.reshape(-1,1))\n",
    "\n",
    "    new_train_X_scaled, X_val = new_train_X_scaled[:int(len(new_train_X_scaled)*0.9)], new_train_X_scaled[int(len(new_train_X_scaled)*0.9):]\n",
    "    new_train_Y_scaled, y_val = new_train_Y_scaled[:int(len(new_train_Y_scaled)*0.9)], new_train_Y_scaled[int(len(new_train_Y_scaled)*0.9):]\n",
    "\n",
    "    \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    model.fit(new_train_X_scaled, new_train_Y_scaled, epochs=100, validation_data=(X_val, y_val), batch_size=10, verbose=1, callbacks=[early_stop])\n",
    "    test_data = []\n",
    "    for i in error_list:\n",
    "        test_data.append(i)\n",
    "    test_data = np.array(test_data)\n",
    "    scaler_test = preprocessing.MinMaxScaler()\n",
    "    test_data_scaled = scaler_test.fit_transform(test_data.reshape(-1,1))\n",
    "    predictions_scaled = model.predict(test_data_scaled)\n",
    "    predictions = scaler_test.inverse_transform(predictions_scaled)\n",
    "    pred_final = predictions + predicted_list\n",
    "    mape = mean_absolute_percentage_error(df_test['Total Confirmed'].to_list(),pred_final[0])\n",
    "    print('Test MAPE: %f' % mape)\n",
    "    evaluate_prediction(pred_final[0],df_test['Total Confirmed'].to_list(),\"ARIMA+ANN\")\n",
    "    df = pd.read_csv('graph.csv')\n",
    "    df['Hybrid ARIMA+ANN'] = pred_final[0]\n",
    "    df.to_csv('graph.csv', index=False)\n",
    "arima_ann(data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_secs = (dt.now() -start).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
